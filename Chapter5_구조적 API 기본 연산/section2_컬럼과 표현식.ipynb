{
 "cells": [
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "1714e434",
   "metadata": {},
   "source": [
    "사용자는 **표현식**으로 DataFrame의 컬럼을 선택,조작,제거할 수 있다. 스파크에선 dataframe의 트랜스포메이션을 사용해야 컬럼 내용에 접근할 수 있다. \n",
    "\n",
    "# 1. 컬럼\n",
    "---\n",
    "\n",
    "컬럼 생성은 col/column 함수를 사용하는 것이 가장 간단하다."
=======
   "id": "ff7a9cb0",
   "metadata": {},
   "source": [
    "사용자는 **표현식** 으로 dataframe의 컬럼을 선택,조작,제거할 수 있다. 이 때, 트랜스포메이션을 사용해야한다.\n",
    "\n",
    "# 1. 칼럼\n",
    "---\n",
    "\n",
    "칼럼은 칼럼명을 카탈로그에 저장된 정보와 비교하기 전까지 미확인 상태로 남는다. \n",
    "\n",
    "## 명시적 컬럼 참조\n",
    "\n",
    "dataframe의 컬럼은 col 메서드로 참조한다. col 메서드를 사용하면 분석기 실행 단계에서 컬럼 확인 절차를 생략한다.\n",
    "\n",
    "```python\n",
    "df.col(\"count\")\n",
    "```\n",
    "\n",
    "<br/><br/>\n",
    "# 2. 표현식\n",
    "---\n",
    "\n",
    "**표현식**이란 무엇일까? 표현식은 dataframe의 row의 여러 값에 대한 트랜스포메이션 집합을 의미한다. 여러 컬럼명을 입력 받아서 \"단일 값\"을 만들기 위해 다양한 표현식을 각 레코드(row)에 적용하는 함수이다. \n",
    "\n",
    "## 표현식으로 컬럼 표현\n",
    "\n",
    "예시를 보자. 아래 두 식은 같은 의미를 가진다.\n",
    "\n",
    "```python\n",
    "\n",
    "expr(\"(((someCol + 5 )* 200)-6)<otherCol\")\n",
    "(((col(\"someCol\")+5)*200)-6)<col(\"otherCol\")\n",
    "\n",
    "```\n",
    "\n",
    "## dataframe 컬럼에 접근하기\n",
    "\n",
    "프로그래밍 방식으로 컬럼에 접근할 땐 columns 속성을 사용하자.\n",
    "\n",
    "```python\n",
    "\n",
    "spark.read.format(\"확장자\").load(\"파일위치/이름\").columns\n",
    "\n",
    "```\n",
    "\n",
    "<br/><br/>\n",
    "# 3. 레코드와 로우\n",
    "---\n",
    "\n",
    "스파크에서 dataframe의 각 로우는 하나의 레코드이다. \n",
    "\n",
    "## 로우 생성하기\n",
    "\n",
    "컬럼을 정했다고 가정하고, 그 컬럼들에 맞는 값으로 row를 따로 생성할 수 있다. "
>>>>>>> 12d22dc37fb05c83ad8a20f1b7fdd93f68579a4a
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "id": "8dda411d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col,column\n\u001b[1;32m----> 3\u001b[0m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\functions.py:152\u001b[0m, in \u001b[0;36mcol\u001b[1;34m(col)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m1.3\u001b[39m)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcol\u001b[39m(col: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    Returns a :class:`~pyspark.sql.Column` based on the given column name.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m    Column<'x'>\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\functions.py:83\u001b[0m, in \u001b[0;36m_invoke_function\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke_function\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m    Invokes JVM function identified by name with args\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m    and wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     jf \u001b[38;5;241m=\u001b[39m _get_jvm_function(name, SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Column(jf(\u001b[38;5;241m*\u001b[39margs))\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
=======
   "execution_count": 5,
   "id": "5adb0839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Row('Hello', None, 1, False)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "Row(\"Hello\",None,1,False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ff232",
   "metadata": {},
   "source": [
    "그리고 로우 데이터는 평상시에 arr[0], arr[1]로 접근하는 것처럼 접근 가능하다. \n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "# 4. DataFrame의 트랜스포메이션\n",
    "---\n",
    "\n",
    "이제 DF를 다루는 방법에 대해 알아보자. 대충 이런 기능이 있다.\n",
    "* 로우나 컬럼 제거\n",
    "* 로우를 컬럼으로 변환하거나 컬럼을 로우로 변환\n",
    "* 로우나 컬럼 추가\n",
    "* 컬럼값을 기준으로 로우 순서 변경\n",
    "\n",
    "이 작업들은 모두 트랜스포메이션으로 변환할 수 있다.\n",
    "\n",
    "## DF 생성하기\n",
    "\n",
    "데이터로부터 DF를 생성해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "641de91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    ".appName(\"Section2\").\\\n",
    "config(\"spark.some.config.option\",\"some-value\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "df = spark.read.format(\"json\").load(\"../data/flight-data/json/2015-summary.json\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efebbed",
   "metadata": {},
   "source": [
    "이번엔 스키마를 직접 설정하고, row 메서드를 이용해 제작해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51ba6239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| some| col|names|\n",
      "+-----+----+-----+\n",
      "|Hello|null|    1|\n",
      "+-----+----+-----+\n",
      "\n"
>>>>>>> 12d22dc37fb05c83ad8a20f1b7fdd93f68579a4a
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "from pyspark.sql.functions import col,column\n",
    "\n",
    "col(\"someColumnName\")\n",
    "column(\"someColumnName\")"
=======
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "MySchema = StructType([\n",
    "    StructField(\"some\",StringType(),True),\n",
    "    StructField(\"col\",StringType(),True),\n",
    "    StructField(\"names\",LongType(),False),    \n",
    "])\n",
    "\n",
    "myRow = Row(\"Hello\",None,1)\n",
    "myDF = spark.createDataFrame([myRow],MySchema)\n",
    "myDF.show()\n"
>>>>>>> 12d22dc37fb05c83ad8a20f1b7fdd93f68579a4a
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "ee265130",
   "metadata": {},
   "source": [
    "위 예제가 왜 에러가 뜨는지 모르겠다. \n",
    "\n",
    "# 2. 표현식\n",
    "---\n",
    "\n",
    "표현식은 DataFrame 레코드의 여러 값에 대한 트랜스포메이션 집합을 의미한다.\n",
    "\n",
    "# 3. 레코드와 로우\n",
    "---\n"
=======
   "id": "084f4ecc",
   "metadata": {},
   "source": [
    "## select와 selectExpr\n",
    "\n",
    "이 메서드들을 사용하면 DF에서 SQL을 사용할 수 있다.  바로 예시를 보자."
>>>>>>> 12d22dc37fb05c83ad8a20f1b7fdd93f68579a4a
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "id": "dd59b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "myRow=Row(\"hello\",None,1,False)"
=======
   "execution_count": 10,
   "id": "c7fe870d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\").show(2)"
>>>>>>> 12d22dc37fb05c83ad8a20f1b7fdd93f68579a4a
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "b088db19",
   "metadata": {},
   "source": [
    "# 4. DataFrame의 트랜스포메이션\n",
    "---"
=======
   "id": "6438d3ba",
   "metadata": {},
   "source": [
    "여러 컬럼도 선택 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10e53c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\",\"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6852da4d",
   "metadata": {},
   "source": [
    "이번엔 expr을 사용해서 컬럼을 참조해보자. 계속 참조라는 말이 나올텐데, 아마 해당 값을 가져온다~ 라는 느낌인 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1a81510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  orgin|\n",
      "+-------+\n",
      "|Romania|\n",
      "|Croatia|\n",
      "+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"ORIGIN_COUNTRY_NAME AS orgin\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a55e1d",
   "metadata": {},
   "source": [
    "바꾸고 다시 되돌리는건 alias를 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d841706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|ORIGIN_COUNTRY_NAME|\n",
      "+-------------------+\n",
      "|            Romania|\n",
      "|            Croatia|\n",
      "+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"ORIGIN_COUNTRY_NAME AS orgin\").alias(\"ORIGIN_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b78dfbc",
   "metadata": {},
   "source": [
    "사실 이게 당장 왜 필요한진 모르겠다.. ㅎ; \n",
    "\n",
    "위 방식을 selectExpr 메서드를 사용하면 편하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b3cf490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|  orgin|ORIGIN_COUNTRY_NAME|\n",
      "+-------+-------------------+\n",
      "|Romania|            Romania|\n",
      "|Croatia|            Croatia|\n",
      "+-------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"ORIGIN_COUNTRY_NAME AS orgin\",\"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d6874",
   "metadata": {},
   "source": [
    "selectExpr은 **새로운 DF를 생성하는 복잡한 표현식을 간단하게 만드는 도구**이다. 예시를 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be54029e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "    \"*\", # 모든 원본 컬럼 포함\n",
    "    \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca016d3b",
   "metadata": {},
   "source": [
    "아, as를 씀으로써 내가 생각하는 표현식을 새롭게 추가해주는 방식으로 간단하게 쓸 수 있구나. 꽤 좋은 방식인 것 같다.\n",
    "\n",
    "## 스파크 데이터 타입으로 변환하기\n",
    "\n",
    "때로는 문자 자체를 하나로 전달해야할 수도 있다. 그 땐, literal을 사용한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d1d6881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(expr(\"*\"),lit(1).alias(\"One\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63245ee2",
   "metadata": {},
   "source": [
    "이렇게 One 컬럼을 만들면, 프로그래밍으로 생성된 변숫값이 특정 컬럼의 값보다 큰지 확인할 때 리터럴을 쓴다는데.. 잘모르겠다. 그냥 표현식으로 하면 되는거 아닌가..?\n",
    "\n",
    "## 컬럼 추가하기\n",
    "\n",
    "withColumn을 사용해서 컬럼을 추가할 수 있다. 사실 나는 selectExpr과 withColumn의 차이점을 잘 모르겠다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9bc5791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"withinCountry\",expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a33304e",
   "metadata": {},
   "source": [
    "## 컬럼명 변경하기\n",
    "\n",
    "withColumnRenamed 메서드로 변경 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af5899a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+-----+\n",
      "|DEST_COUNTRY_NAME| origin|count|\n",
      "+-----------------+-------+-----+\n",
      "|    United States|Romania|   15|\n",
      "|    United States|Croatia|    1|\n",
      "+-----------------+-------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"ORIGIN_COUNTRY_NAME\",\"origin\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6156e4a",
   "metadata": {},
   "source": [
    "## 컬럼 제거하기\n",
    "drop() 메서드를 이용해 제거 가능하다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c09172a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab771fb",
   "metadata": {},
   "source": [
    "## 컬럼의 데이터 타입 변경하기\n",
    "가끔 데이터 타입을 형변환해야할 때가 있다. cast 메서드를 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fd1de13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True), StructField('count2', StringType(), True)])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"count2\",col(\"count\").cast(\"string\")).schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3fefc0",
   "metadata": {},
   "source": [
    "## 로우 필터링하기\n",
    "\n",
    "컬럼의 표현식에 해당하는 로우들만 출력하거나 출력하지 않을 수 있다. where나 filter 메서드를 쓰면 된다. \n",
    "\n",
    "두 메서드는 작성하는 방법만 다르고 똑같이 동작한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca28d9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"count < 2\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09c39958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"count\")<2).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8576e87d",
   "metadata": {},
   "source": [
    "AND로 여러 필터를 적용할 땐 표현식을 작성한 where 메서드를 연결해서 스파크가 알아서 판단하도록 맡기자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1092159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") == \"Croatia\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5c723e",
   "metadata": {},
   "source": [
    "근데 그냥 where(\"ORIGIN_COUNTRY_NAME == Croatia\") 하니까 안된다. col 또는 expr을 쓰는 것은 뭐랄까 그 컬럼 자체를 참조한단 의미인 것 같다. 그래서 참조된 컬럼에서 여러 조건문을 쓰는거다. 앞으로 이것을 자주 이용해 습관화하자.\n",
    "\n",
    "## 고유한 로우 얻기\n",
    "\n",
    "컬럼에서 중복되지 않은 값을 추출하기 위해서 distinct 메서드를 사용하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b32edb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "| ORIGIN_COUNTRY_NAME|\n",
      "+--------------------+\n",
      "|            Paraguay|\n",
      "|              Russia|\n",
      "|            Anguilla|\n",
      "|             Senegal|\n",
      "|              Sweden|\n",
      "|            Kiribati|\n",
      "|              Guyana|\n",
      "|         Philippines|\n",
      "|           Singapore|\n",
      "|            Malaysia|\n",
      "|                Fiji|\n",
      "|              Turkey|\n",
      "|             Germany|\n",
      "|              Jordan|\n",
      "|               Palau|\n",
      "|Turks and Caicos ...|\n",
      "|              France|\n",
      "|              Greece|\n",
      "|British Virgin Is...|\n",
      "|              Taiwan|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f19518",
   "metadata": {},
   "source": [
    "## 무작위 샘플 만들기\n",
    "\n",
    "sample 메서드를 사용해서 DF에서 표본 데이터 추출 비율을 지정할 수 있고, 복원 추출이나 비복원 추출의 사용 여부를 지정할 수 있다. \n",
    "여부는 withReplacement로 결정한다.\n",
    "\n",
    "* 복원 추출이란, 뽑은 것은 다시 넣고 랜덤하게 섞어서 하나를 뽑는 방식\n",
    "* 비복원 추출이란, 뽑은 것은 다시 넣지 않고 랜덤하게 섞어서 하나를 뽑는 방식\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea339694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 5\n",
    "withReplacement = True\n",
    "fraction = 0.5\n",
    "df.sample(withReplacement,fraction,seed).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fdc42f",
   "metadata": {},
   "source": [
    "## 임의 분할하기\n",
    "\n",
    "원본 DF를 임의의 크기로 분할할 때 유용하다. 머신러닝 알고리즘에서 학습/검증/테스트 셋을 만들 때 주로 사용한다.\n",
    "비율을 직접 설정하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5aecc816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_split = df.randomSplit([0.25,0.75],seed)\n",
    "df_split[0].count() > df_split[1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425651d7",
   "metadata": {},
   "source": [
    "## 로우 합치기와 추가하기\n",
    "\n",
    "DF는 불변성을 가지므로, DF에 레코드를 추가하는 작업은 DF를 변경하는 것이므로 불가능하다. 여태까지 했던 withColumn 같은건 뭐지..? \n",
    "\n",
    "어쨌든 그래서 union 메서드를 사용해서 기존의 DF와 새로운 DF를 합치는 방식으로 해야한다. 위 아래를 붙이는 것이니, **당연히 스키마와 컬럼이 같아야한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b1007789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "|    United States|             Cyprus|    1|\n",
      "|    United States|            Estonia|    1|\n",
      "|    United States|          Lithuania|    1|\n",
      "|    United States|           Bulgaria|    1|\n",
      "|    United States|            Georgia|    1|\n",
      "|    United States|            Bahrain|    1|\n",
      "|    United States|   Papua New Guinea|    1|\n",
      "|    United States|         Montenegro|    1|\n",
      "|    United States|            Namibia|    1|\n",
      "|    New country 2|    other country 3|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "schema = df.schema\n",
    "newRows = [\n",
    "    Row(\"New country\",\"other country\",5),\n",
    "    Row(\"New country 2\",\"other country 3\",1),\n",
    "    \n",
    "]\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "newDF = spark.createDataFrame(parallelizedRows,schema)\n",
    "\n",
    "df.union(newDF)\\\n",
    ".where(col(\"count\")==1)\\\n",
    ".where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ba4d55",
   "metadata": {},
   "source": [
    "## 로우 정렬하기\n",
    "sort와 orderBy 메서드를 사용해서 정렬할 수 있다. 두 메서드는 완전히 같은 방식으로 동작한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9968f3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"count\",\"DEST_COUNTRY_NAME\").show(5)\n",
    "df.sort(\"count\",\"DEST_COUNTRY_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce767415",
   "metadata": {},
   "source": [
    "코딩에서 sort(key=lambda x :(x[0],-x[1])) 처럼 서로 다른 방향으로 정렬할 땐, desc,asc 함수를 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4638dabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc,asc\n",
    "df.orderBy(expr(\"count desc\")).show(2)\n",
    "df.orderBy(col(\"count\").desc(),col(\"DEST_COUNTRY_NAME\").asc()).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff7889",
   "metadata": {},
   "source": [
    "## repartition과 coalesce\n",
    "\n",
    "자주 필터링하는 칼럼을 기준으로 데이터를 분할해서 최적화할 수 있다. 이것을 통해 파티셔닝 스키마와 파티션 수를 포함해 클러스터 전반의 물리적 데이터 구성을 제어할 수 있다.\n",
    "\n",
    "repartition 메서드를 호출하면 무조건 전체 데이터를 셔플한다. **나중에 사용할 파티션 수가 현재 파티션 수보다 많거나 칼럼을 기준으로 파티션을 만들 경우에만 사용하자.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ca598640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997f17f",
   "metadata": {},
   "source": [
    "**특정 컬럼을 기준으로 자주 필터링 한다면 자주 필터링되는 컬럼을 기준으로 파티션을 재분배하는 것이 좋다.** 선택적으로 파티션 수를 지정할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42180a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5,col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f96a5fb",
   "metadata": {},
   "source": [
    "coalesce 메서드는 전체 데이터를 셔플하지 않고 파티션을 병합하려는 경우에만 사용한다. 다음 예제는 DEST_COUNTRY_NAME를 기준으로 셔플을 수행해 5개의 파티션으로 나누고, 전체 데이터를 다시 병합하는 예제이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d48b7ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5,col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd2254",
   "metadata": {},
   "source": [
    "## 드라이버로 로우 데이터 수집하기\n",
    "\n",
    "스파크는 드라이버에서 클러스터 상태 정보를 유지한다. 그래서 실제로 클러스터들이 있을 때, 로우 데이터는 분산되어 있어서 연산을 할 때 병렬로 처리가 가능하다. \n",
    "그러나, 이 로우 데이터들을 드라이버로 수집해야하는 경우가 있다. 대표적으로 우리처럼 로컬 환경에서 데이터를 다룰 때이다. \n",
    "\n",
    "여태 몇번 사용했었던 메서드들이 그 예시이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1ee41c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62),\n",
       " Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588),\n",
       " Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF = df.limit(10) # 상위 10개만 뽑기\n",
    "collectDF.take(5)\n",
    "collectDF.show()\n",
    "collectDF.show(5,False)\n",
    "collectDF.collect() # 분산되어 있던 로우데이터들을 모으는 메서드이다. "
>>>>>>> 12d22dc37fb05c83ad8a20f1b7fdd93f68579a4a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "55c5a309",
=======
   "id": "9168d084",
>>>>>>> 12d22dc37fb05c83ad8a20f1b7fdd93f68579a4a
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
