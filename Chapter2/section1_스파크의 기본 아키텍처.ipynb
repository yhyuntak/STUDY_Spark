{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e00eff0",
   "metadata": {},
   "source": [
    "한대의 컴퓨터는 대규모 정보를 연산할만한 자원이나 성능을 갖지 못한다. 컴퓨터 클러스터는 여러 컴퓨터의 자원을 모아 하나의 컴퓨터처럼 사용할 수 있게 만들어준다. 이런 컴퓨터 클러스터에서 작업을 조유해주는 프레임워크가 **스파크**이다. \n",
    "\n",
    "스파크가 연산에 사용할 클러스터는 다음과 같은 클러스터 매니저에서 관리한다.\n",
    "\n",
    "* 스파크 스탠드얼론 클러스터 매니저\n",
    "* 하둡 YARN\n",
    "* 메소스\n",
    "\n",
    "사용자는 클러스터 매니저에 스파크 애플리케이션을 제출한다. 그럼 클러스터 매니저는 애플리케이션 실행에 필요한 자원을 할당하며 우리는 할당받은 자원으로 작업을 처리하게 된다.\n",
    "\n",
    "\n",
    "# 1. 스파크 애플리케이션\n",
    "---\n",
    "\n",
    "스파크 애플리케이션은 드라이버 프로세스와 다수의 익스큐터 프로세스로 구성된다. \n",
    "\n",
    "* 드라이버 프로세스의 역할 \n",
    "    * 스파크 애플리 케이션 정보의 유지 관리\n",
    "    * 사용자 입력에 대한 응답\n",
    "    * 전반적인 익스큐터 프로세스의 작업과 관련된 분석,배포, 스케쥴링 역할 수행\n",
    "    \n",
    "* 익스큐터 역할\n",
    "    * 드라이버 프로세스가 할당한 작업을 수행\n",
    "    * 진행 상황을 드라이버 노드에 보고\n",
    "   \n",
    "스파크는 클러스터 모드와 로컬 모드를 모두 지원한다. 로컬 모드는 드라이버와 익스큐터를 단일 머신에서 스레드 형식으로 실행한다. 그래서 나처럼 혼자 실습하는 사람도 사용 가능하다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "# 2. 스파크의 다양한 언어 API\n",
    "---\n",
    "\n",
    "사용자는 스파크 코드를 실행하기 위해 **SparkSession 객체를 진입점**으로 사용할 수 있다. 따라서 사용자가 SparkSession 객체를 사용해서 파이썬이나 R로 코드를 작성하면, 스파크는 익스큐터의 JVM에서 실행할 수 있는 코드로 변환해준다.\n",
    "\n",
    "<br/><br/>\n",
    "# 3. 스파크 API\n",
    "---\n",
    "\n",
    "스파크는 저수준의 비구조적 API와 고수준의 구조적 API를 제공하므로 다양한 언어로 스파크를 사용할 수 있다.\n",
    "\n",
    "<br/><br/>\n",
    "# 5. SparkSession\n",
    "---\n",
    "\n",
    "스파크 애플리케이션은 SparkSession이라 불리는 드라이버 프로세스로 제어한다. 매번 영어로 쓰기 힘들어서 **스파크 세션**이라고 부르겠다. 스파크세션 객체는 사용자가 정의한 처리 명령을 클러스터에서 실행한다. **하나의 스파크 세션은 하나의 스파크 애플리케이션에 대응**한다. \n",
    "\n",
    "책에는 주로 {스파크 tgz 압축 해제 폴더 경로}에 들어가서 ./bin/pyspark를 사용해서 설명을 한다. 책이 집필될 2018년에는 pyspark 라이브러리가 정립되어있지 않아서 그런 듯하다. 그러나 2022년인 지금은 다 완료가 되었을테니.. 나는 최대한 파이썬 코드로만 작성을 해볼 예정이다. pyspark에 대한 정보는 [공식 사이트](https://spark.apache.org/docs/3.2.1/api/python/reference/index.html)를 참고하자.\n",
    "\n",
    "기본적으로 [인터넷에 있는](https://spark.apache.org/docs/3.2.1/api/python/getting_started/quickstart_df.html) 스파크 세션 객체를 생성하는 예제는 다음과 같다. (아래 예시는 2.2.0의 구버젼의 예시다..~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "74406e9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.6:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fadf016fee0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f43561",
   "metadata": {},
   "source": [
    "builder를 통해 여러 클래스 속성들을 지정해줘야한다. \n",
    "* appName은 말그대로 애플리케이션의 이름을 지정해 주는 것이다\n",
    "* config는 config 옵션들을 지정해주는데, 파라미터들은 순서대로 key,value,conf이다. key에 해당하는 값을 value로 지정한다는 의미 같다.\n",
    "* getOrCreate은 존재하는 스파크 세션을 가져오거나 builder로 세팅된 옵션들로 새로운 세션을 만든다. \n",
    "\n",
    "더 다양한 빌더 옵션들은 공식 사이트를 참고하도록 하자.\n",
    "\n",
    "<br/><br/>\n",
    "# 6. DataFrame\n",
    "---\n",
    "\n",
    "데이터프레임은 가장 대표적인 구조적 API이다. \n",
    "\n",
    "먼저 스파크 세션의 데이터프레임의 예시를 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a3dfdb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myRange = spark.range(0,1000).toDF(\"number\")\n",
    "myRange.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e6878",
   "metadata": {},
   "source": [
    "0-999까지의 값이 row로, \"number\"라는 이름으로 col이 생성됬다. 숫자 0-999를 **분산 컬렉션**을 나타낸다고 한다. 클러스터 모드에서 코드 예제를 실행하면, 숫자 범위가 쪼개져서 서로 다른 익스큐터에 할당된다고 한다. 즉, 스파크의 데이터프레임은 수천 대의 컴퓨터에 분산이 가능해진다. 따라서 몇 억개가 되는 데이터도 처리가 가능한 것 같다. 강력한 능력이다..!\n",
    "\n",
    "* 파이썬의 pandas 라이브러리와 호환이 가능한 것 같다.\n",
    "\n",
    "## 1) 파티션\n",
    "\n",
    "스파크는 모든 익스큐터가 병렬로 작업을 수행할 수 있도록 파티션이라 불리는 청크 단위로 데이터를 분할한다. 파티션은 클러스터의 물리적 머신에 존재하는 row의 집합을 의미한다. (병렬성을 위해 파티션과 익스큐터는 서로 많은 갯수를 지녀야하는 것 같다.) 1부터 10의 값을 갖는 열벡터가 있다면, 1,2,...가 하나의 파티션이다.\n",
    "\n",
    "<br/><br/>\n",
    "# 7. 트랜스포메이션\n",
    "---\n",
    "\n",
    "스파크의 핵심 데이터 구조는 **불변성**을 가진다. 그래서 한번 생성하면 변경이 불가능하지만, **트랜스포메이션**을 사용하여 **나는 이 데이터를 이렇게 할꺼야!! 그러니까 알고있어!!** 라는 느낌으로 데이터 프레임에 변경 방법을 예고할 수 있다. **액션**을 호출하지 않으면 트랜스포메이션을 수행하지 않는다. \n",
    "\n",
    "예제를 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ef7dfb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "divisBy2 = myRange.where(\"number % 2 = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3a2631",
   "metadata": {},
   "source": [
    "위 코딩 자체로는 아직 divisBy2에 조건문에 해당하는 값들이 저장되어 있지 않다. \n",
    "\n",
    "트랜스포메이션에는 2가지 유형이 있다.\n",
    "\n",
    "1. 좁은 의존성(narrow dependency) <br/>\n",
    "    \n",
    "    하나의 입력 파티션이 하나의 출력 파티션에만 영향을 미친다. 위의 예시가 좁은 의존성을 가르킨다. 조건에 의해 각각의 파티션이 각각의 출력에 응답하는 형식이다. \n",
    "    \n",
    "2. 넓은 의존성(wide dependency) <br/>\n",
    "\n",
    "    하나의 입력 파티션이 여러 출력 파티션에 영향을 미치는 것이다. 예를 들면, 스파크가 클러스터에서 파티션을 교환하는 **셔플**이라는 방법이 있다고 한다.\n",
    "    \n",
    "## 1) 지연 연산(lazy evaluation)\n",
    "\n",
    "스파크가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식을 의미한다. 아까 트랜스포메이션에서 액션을 호출하지 않으면 수행하지 않는다고 했는데, 수행하지 않고 기다리는 것을 의미한다. 스파크는 코드를 실행하는 마지막 순간까지 대기하면서 **트랜스포메이션의 실행 계획**을 생성하다가 한번에 컴파일을 하며 전체 데이터 흐름을 최적화한다고 한다.\n",
    "\n",
    "<br/><br/>\n",
    "# 8. 액션\n",
    "---\n",
    "\n",
    "트랜스포메이션을 이제 실행하는 단계이다. 가장 단순한 액션은 count()이다. pandas를 사용해봤다면 익숙한 메서드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a9b8f282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisBy2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d082a12",
   "metadata": {},
   "source": [
    "개수를 세는 것 뿐만 아니라 3개의 액션이 더 추가적으로 실행된다고 한다. \n",
    "\n",
    "* 콘솔에서 데이터를 보는 액션\n",
    "* 각 언어로 된 네이티브 객체에 데이터를 모으는 액션\n",
    "* 출력 데이터소스에 저장하는 액션\n",
    "\n",
    "액션을 지정하면 스파크 잡(job)이 시작된다. 스파크 잡의 트랜스포메이션 수행 방식은 **좁은->넓은** 인 것 같다. \n",
    "\n",
    "count()를 예시로 들어보자\n",
    "> 필터(좁은 트랜스포메이션)을 수행한 후, 파티션별로 레코드 수를 카운트(넓은 트랜스포메이션)\n",
    "\n",
    "<br/><br/>\n",
    "# 9. 스파크 UI\n",
    "---\n",
    "\n",
    "스파크 UI는 스파크 잡의 진행 상황을 모니터링할때 사용하낟. 드라이버 노드의 4040포트로 접속할 수 있다. 지금은 로컬 모드에서 사용하니 localhost:4040으로 접속이 가능하다. \n",
    "\n",
    "<br/><br/>\n",
    "# 10. 종합 예제\n",
    "---\n",
    "\n",
    "실제 데이터를 가지고 놀아보자! 미국 교통통계국의 항공운항 데이터 중 일부를 분석해보자. [링크](https://github.com/FVBros/Spark-The-Definitive-Guide/tree/master/data/flight-data/csv)에 있는 2015년 데이터를 사용했다. \n",
    "\n",
    "데이터는 스파크 세션의 DataFrameReader 클래스를 사용해서 읽는다. 이때 특정 파일 포맷과 몇 가지 옵션을 함께 설정한다. 예제에선 스파크 DataFrame의 스키마 정보(col과 col 타입)를 알아내는 스키마 추론(schema inferenece)기능을 사용한다.\n",
    "\n",
    "여기서 csv파일을 읽는 것 자체도 좁은 트랜스포메이션의 일종이다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "249bcf43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015 = spark.read\\\n",
    "                    .option(\"inferSchema\",\"true\")\\\n",
    "                    .option(\"header\",\"true\")\\\n",
    "                    .csv(\"../data/flight-data/csv/2015-summary.csv\")\n",
    "\n",
    "flightData2015.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb191c",
   "metadata": {},
   "source": [
    "그럼 스키마 추론을 하지 않으면 어떻게 될까?? 궁금해서 해봤더니 스키마 추론보단 header의 여부가 더 중요한거 같다..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6a9732aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='DEST_COUNTRY_NAME', _c1='ORIGIN_COUNTRY_NAME', _c2='count'),\n",
       " Row(_c0='United States', _c1='Romania', _c2='15'),\n",
       " Row(_c0='United States', _c1='Croatia', _c2='1'),\n",
       " Row(_c0='United States', _c1='Ireland', _c2='344'),\n",
       " Row(_c0='Egypt', _c1='United States', _c2='15')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_schema_inferenece = spark.read\\\n",
    "                    .option(\"inferSchema\",\"true\")\\\n",
    "                    .csv(\"../data/flight-data/csv/2015-summary.csv\")\n",
    "not_schema_inferenece.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f357a24",
   "metadata": {},
   "source": [
    "정렬을 해보자. sort() 메서드를 사용하면 되는데, 이것 또한 넓은 트랜스포메이션의 일종이다. 즉, read() 메서드 -> sort() 메서드 는 트랜스포메이션이고  take(5)가 액션이 되는거다. \n",
    "\n",
    "여기서 내가 트랜스포메이션들을 사용해서 **실행 계획**을 어떻게 짜놨는지 보고 싶으면 explain()을 사용하면 된다. 그럼 아래로 내려갈수록 단계별로 트랜스포메이션이 어떻게 되는지 보여준다. 예제를 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "618cee83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#1089 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#1089 ASC NULLS FIRST, 100), ENSURE_REQUIREMENTS, [id=#2186]\n",
      "      +- Filter (((isnotnull(count#1089) AND isnotnull(DEST_COUNTRY_NAME#1087)) AND (count#1089 = 0)) AND (DEST_COUNTRY_NAME#1087 = United States))\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#1087,ORIGIN_COUNTRY_NAME#1088,count#1089] Batched: false, DataFilters: [isnotnull(count#1089), isnotnull(DEST_COUNTRY_NAME#1087), (count#1089 = 0), (DEST_COUNTRY_NAME#1..., Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/hyuntak/project/STUDY_Spark/data/flight-data/csv/2015-summa..., PartitionFilters: [], PushedFilters: [IsNotNull(count), IsNotNull(DEST_COUNTRY_NAME), EqualTo(count,0), EqualTo(DEST_COUNTRY_NAME,Unit..., ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort('count').where(\"count = 0\").where(\"DEST_COUNTRY_NAME = 'United States'\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40866437",
   "metadata": {},
   "source": [
    "explain을 보니까 이제 좀 스파크가 이해가 된다. 책의 75페이지를 보면 스파크가 실행 계획을 통해 전체 데이터 흐름을 최적화하는 엄청난 강점을 보여준다는데, 이게 explain() 메서드를 써보니까 이해가 된다. 위에 실행 계획에서 sort()와 where()가 각각 +- Sort ... +- Filter 로 적혀있다. 그리고 제일 마지막이 핵심이다. 각각의 트랜스포메이션으로 조건절 푸시다운(predicate pushdown)이 일어나서 **딱 원하는 데이터만을 추출**하는 최적의 데이터 처리 방법을 보이는 것이다..\n",
    "\n",
    "이게 무슨 말이냐면 .. 원래 python에서는 다음과 같이 데이터가 처리될 것이다. 전체 데이터를 N, count = 0인 데이터를 M, count=0이면서 목적지가 미국인 데이터가 L이라고 하자.\n",
    "\n",
    "1. sort() 메서드를 통해 전체 데이터를 한번 정렬 O(N)\n",
    "2. 전체 데이터에서 조건문 .where(\"count = 0\")에 의해 데이터를 한번 찾기 O(N)\n",
    "3. 찾아진 데이터에서 .where(\"DEST_COUNTRY_NAME = 'United States'\")에 의해 데이터 찾기 O(M)\n",
    "\n",
    "으로 시간 복잡도가 정확하진 않지만.. 저런 느낌으로 여러번 데이터가 읽혀질 것이다. 근데 스파크를 사용하면 실행 계획을 통해서 조건절 푸시다운이 일어나서 **딱 원하는 데이터를 마지막에 읽어서 추출**하는 시스템이 만들어져 데이터를 읽는 과정이 최적화 됨을 얘기하는 것 같다. 이건 완전 틀린 설명일 것 같지만, 뭔가 이런 느낌!? 인 것 같아서 써봤다!! 핵심은 스파크는 **액션이 호출되기 전까지 데이터를 읽지 않는다는 것이다.** \n",
    "\n",
    "이제 실행 계획을 시작하기 위해 액션을 호출하자. 스파크는 셔플 수행 시 기본적으로 200개의 셔플 파티션을 생성한다. 이것은 5개로 줄여보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "589134c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='Malta', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Saint Vincent and the Grenadines', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Gibraltar', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"100\")\n",
    "flightData2015.sort(\"count\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70076ff5",
   "metadata": {},
   "source": [
    "SQL 쿼리를 사용하고 싶으면 데이터 프레임을 createOrReplaceTempView(name)으로 테이블 또는 뷰를 생성해서 이용할 수 있다. SQL쿼리를 이용하나 그냥 스파크 데이터 프레임를 이용하나 동일한 실행 계획이 컴파일 되는 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f33e4624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#1087], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#1087, 100), ENSURE_REQUIREMENTS, [id=#2208]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#1087], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#1087] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/hyuntak/project/STUDY_Spark/data/flight-data/csv/2015-summa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#1087], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#1087, 100), ENSURE_REQUIREMENTS, [id=#2221]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#1087], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#1087] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/hyuntak/project/STUDY_Spark/data/flight-data/csv/2015-summa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")\n",
    "\n",
    "# SQL쿼리\n",
    "sqlWay = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME,count(1)\n",
    "from flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "\n",
    "# 파이썬 코드\n",
    "dataFrameway = flightData2015.groupBy(\"DEST_COUNTRY_NAME\").count()\n",
    "\n",
    "sqlWay.explain()\n",
    "dataFrameway.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f87bb",
   "metadata": {},
   "source": [
    "스파크는 빅데이터 문제를 빠르게 해결하는데 필요한 많은 함수를 제공한다. 예를 들어 max 함수를 사용해보자. max 함수는 필터링을 수행해 단일 로우를 결과로 반환하는 트랜스포메이션이다. 더 많은 함수들에 대해선 [공식 사이트의 API 레퍼런스](https://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.sql.functions.abs.html)를 참고하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c051966a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(max(count)=370002)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL쿼리\n",
    "print(spark.sql(\"SELECT max(count) from flight_data_2015\").take(1))\n",
    "\n",
    "# 데이터 프레임 이용\n",
    "from pyspark.sql.functions import max\n",
    "(flightData2015.select(max(\"count\")).take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b663367b",
   "metadata": {},
   "source": [
    "이번엔 다중 트랜스포메이션 쿼리를 다뤄보자. 직관적인 SQL쿼리부터 시작하자. SQL 쿼리를 써본 적 없지만 몇번 코드를 사용해보니 알겠다. \n",
    "\n",
    "* SELECT는 column 명을 지정하고 해당 데이터를 col값으로 넣는 것 같다.\n",
    "* FROM은 테이블/뷰 이름을 의미한다.\n",
    "* GROUP BY는 column 명의 분류들을 그룹화해서 개수를 카운팅한다.\n",
    "* ORDER BY {기준} {차순}는 {기준}으로 {차순}대로 정렬한다.\n",
    "* LIMIT은 take,head와 같다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d6638d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL 쿼리 이용\n",
    "maxSql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336dfc76",
   "metadata": {},
   "source": [
    "이번엔 데이터 프레임 구문을 보자. 조금 구현과 정렬 방식이 다르다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "59f5da15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "withColumnRenamed와 sort를 뺐을 경우\n",
      "+-----------------+----------+\n",
      "|DEST_COUNTRY_NAME|sum(count)|\n",
      "+-----------------+----------+\n",
      "|           Panama|       510|\n",
      "|       Cape Verde|        20|\n",
      "|        Hong Kong|       332|\n",
      "|         Anguilla|        41|\n",
      "|           Russia|       176|\n",
      "+-----------------+----------+\n",
      "\n",
      "withColumnRenamed와 sort를 추가했을 경우\n",
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# withColumnRenamed와 sort를 뺐을 경우\n",
    "print(\"withColumnRenamed와 sort를 뺐을 경우\")\n",
    "flightData2015.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\").limit(5).show()\n",
    "\n",
    "# withColumnRenamed와 sort를 추가했을 경우 -> SQL 쿼리와 같다.\n",
    "print(\"withColumnRenamed와 sort를 추가했을 경우\")\n",
    "flightData2015.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\").withColumnRenamed(\"sum(count)\",\"destination_total\")\\\n",
    ".sort(desc(\"destination_total\")).limit(5).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e638d16",
   "metadata": {},
   "source": [
    "코드를 해석하기 위해서 withColumnRenamed와 sort를 뺀 경우와 넣은 경우를 확인했다. 대충 이제 감이 오기 시작한다!!\n",
    "\n",
    "show()가 액션이고 나머지는 전부 트랜스포메이션이다. 실행 계획은 **트랜스포메이션의 지향성 비순환 그래프(directed acyclic graph,DAG)**이고, 액션이 호출되야 결과를 만들어내는 것을 잊지 말자. 그리고 DAG의 각 단계는 불변성을 가진 신규 데이터 프레임을 생성한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
