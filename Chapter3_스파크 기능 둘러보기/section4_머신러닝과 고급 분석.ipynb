{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24508e2",
   "metadata": {},
   "source": [
    "# 4. 머신러닝과 고급 분석\n",
    "---\n",
    "\n",
    "스파크는 내장된 머신러닝 알고리즘 라이브러리인 MLlib을 사용해 대규모 머신러닝을 수행할 수 있다. MLlib을 사용하면 대용량 데이터를 대상으로 전처리,멍잉(munging),모델 학습 및 예측을 할 수 있다. 구조적 스트리밍에서 예측을 할 때도 MLlib에서 학습한 다양한 모델을 사용할 수 있다. \n",
    "\n",
    "k-means로 간단한 군집화 예시를 봐보자. 데이터는 section2에서 사용했던 소매 데이터 셋을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c470286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SparkSession for section4\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "staticDataFrame = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"../data/retail-data/by-day/*.csv\")\n",
    "\n",
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "staticSchema = staticDataFrame.schema\n",
    "\n",
    "staticDataFrame.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e428abd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e024b",
   "metadata": {},
   "source": [
    "MLlib을 사용하기 위해선 수치형 데이터가 필요하다. 이번 예제에선 3개의 컬럼 데이터들 [UnitPrice,Quantity,day_of_week_encoded]을 사용할 것이다. day_of_week_encoded는 InvoiceDate를 요일로 바꿔서 원-핫 인코딩을 거친 데이터다. 이 과정을 한번 살펴보자.\n",
    "\n",
    "먼저 새로운 column \"day_of_week\"을 생성하기 위해서 DataFrame의 withColumn 메서드를 사용하자. 그리고 날짜의 포맷을 date_format()으로 바꿔주자. 자세한 설명들은 공식 API 레퍼런스를 참고하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0b5b422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|day_of_week|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|     Monday|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "preppedDataFrame = staticDataFrame\\\n",
    "  .na.fill(0)\\\n",
    "  .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\\\n",
    "  .coalesce(5)\n",
    "\n",
    "preppedDataFrame.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131dc73d",
   "metadata": {},
   "source": [
    "데이터를 학습/테스트 데이터셋으로 분리하자. 특정 구매가 이루어진 날짜(2011-07-01)를 기준으로 직접 분리하자. 이 분리방법은 사실 좋은 방법은 아니지만, 일단 간단히 맛만 보자. 그리고 지연 연산을 끝냈으니 액션을 호출하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa68da33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set :  245903\n",
      "test set :  296006\n"
     ]
    }
   ],
   "source": [
    "trainDataFrame = preppedDataFrame\\\n",
    "  .where(\"InvoiceDate < '2011-07-01'\")\n",
    "testDataFrame = preppedDataFrame\\\n",
    "  .where(\"InvoiceDate >= '2011-07-01'\")\n",
    "\n",
    "print(\"train set : \",trainDataFrame.count())\n",
    "print(\"test set : \",testDataFrame.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238e749",
   "metadata": {},
   "source": [
    "스파크 MLlib은 일반적인 트랜스포메이션을 자동화하는 다양한 트랜스포메이션을 제공한다. 먼저 **StringIndexer()**를 보자. 이 함수는 string의 컬럼 데이터를 정수화해준다. 예를 들면, 월요일 -> 1, 화요일 -> 2 .. 일요일 -> 7 이런 식으로 말이다. \n",
    "\n",
    "그러나 정수화만 진행하면 loss function에 들어서면서 수치에 따라 중요도가 바뀔 수 있다. 그래서 함수 **OneHotEncoder()**를 통해 원-핫 인코딩을 실행할 준비를 하자.\n",
    "\n",
    "마지막으로 [UnitPrice,Quantity,day_of_week_encoded]를 벡터로 만들어주는 함수 VectorAssembler()도 추가하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0376bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer()\\\n",
    "  .setInputCol(\"day_of_week\")\\\n",
    "  .setOutputCol(\"day_of_week_index\")\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder()\\\n",
    "  .setInputCol(\"day_of_week_index\")\\\n",
    "  .setOutputCol(\"day_of_week_encoded\")\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler()\\\n",
    "  .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n",
    "  .setOutputCol(\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51148acf",
   "metadata": {},
   "source": [
    "이제 모든 함수 준비가 끝났으니 **파이프라인을 만들어서 나중에 데이터가 들어왔을 때, 자동으로 프로세스를 거쳐 변환되도록 준비하자.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2bb0ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transformationPipeline = Pipeline()\\\n",
    "  .setStages([indexer, encoder, vectorAssembler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569e78a",
   "metadata": {},
   "source": [
    "사이킷런을 사용한 적이 있다면 잘 알겠지만, string 데이터를 정수로 인덱싱하려면 어떤 string이 존재하는지 알아야한다. 그것을 fit()메서드로 진행하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dffa9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedPipeline = transformationPipeline.fit(trainDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2f7c63",
   "metadata": {},
   "source": [
    "데이터에 맞게 fitting을 했다면, 학습/테스트 셋을 변환하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bd56970",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTraining = fittedPipeline.transform(trainDataFrame)\n",
    "transformedTest = fittedPipeline.transform(testDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea11ad",
   "metadata": {},
   "source": [
    "책에 데이터 캐싱을 설명하기 위해서 파이프라인 구성 과정에서 데이터 캐싱 과정을 제외했다는데.. 아직 무슨 얘긴진 모르겠다.\n",
    "동일한 트랜스포메이션을 반복할 수 없으니 모델에 하이퍼 파라미터 튜닝 값을 적용한다는데.. **page 103~104에 써있는 말이 아직 이해가 안간다.**\n",
    "\n",
    "이제 Kmeans 모델을 생성하고 학습을 진행하자. MLlib에선 명칭 규칙이 있다. 학습 전 모델을 algorithm으로 학습 후 모델을 algorithmModel이라고 부른다고 한다. 학습 및 예측을 하는 방법은 사이킷런과 비슷하다. \n",
    "\n",
    "책에 내용이 조금 부실하거나 스파크 버전이 바뀌면서 고쳐야 되는 것들을 수정했다. 참고 : [공식 API 레퍼런스](https://spark.apache.org/docs/latest/ml-clustering.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "727c4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator # 책의 computeCost를 대체한다.\n",
    "kmeans = KMeans()\\\n",
    "  .setK(20)\\\n",
    "  .setSeed(1)\n",
    "\n",
    "# 학습\n",
    "kmModel = kmeans.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f156fde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9326149512542127\n",
      "Cluster Centers: \n",
      "[3.6340258  5.63459494 0.19581065 0.1938833  0.18023509 0.17188605\n",
      " 0.149228  ]\n",
      "[1.0400e+00 7.4215e+04 0.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00]\n",
      "[ 1.0400e+00 -7.4215e+04  0.0000e+00  1.0000e+00  0.0000e+00  0.0000e+00\n",
      "  0.0000e+00]\n",
      "[ 3.897e+04 -1.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
      "  1.000e+00]\n",
      "[7.85972222e-01 1.14452778e+03 2.08333333e-01 2.50000000e-01\n",
      " 9.72222222e-02 2.22222222e-01 1.66666667e-01]\n",
      "[ 5.43415e+03 -1.00000e+00  0.00000e+00  1.25000e-01  3.75000e-01\n",
      "  0.00000e+00  5.00000e-01]\n",
      "[ 1.6670865e+04 -1.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  1.0000000e+00  0.0000000e+00]\n",
      "[3.8500e-01 4.4435e+03 2.5000e-01 2.5000e-01 0.0000e+00 0.0000e+00\n",
      " 5.0000e-01]\n",
      "[1.11846743e+00 5.25536398e+02 3.10344828e-01 1.95402299e-01\n",
      " 1.76245211e-01 2.06896552e-01 8.81226054e-02]\n",
      "[9.74117647e-01 2.41200000e+03 2.35294118e-01 1.76470588e-01\n",
      " 2.35294118e-01 1.17647059e-01 1.76470588e-01]\n",
      "[ 7.5000e-03 -9.4045e+03  2.5000e-01  7.5000e-01  0.0000e+00  0.0000e+00\n",
      "  0.0000e+00]\n",
      "[1.34538732e+00 1.96409155e+02 2.48591549e-01 2.38028169e-01\n",
      " 1.22535211e-01 2.00704225e-01 1.53521127e-01]\n",
      "[ 1.54467342 68.05678486  0.20927753  0.21794188  0.13116502  0.21754199\n",
      "  0.15249267]\n",
      "[ 1.3524695e+04 -5.0000000e-01  0.0000000e+00  1.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "[ 1.39452044e+03 -2.22222222e-02  1.33333333e-01  1.55555556e-01\n",
      "  4.44444444e-01  4.44444444e-02  2.22222222e-01]\n",
      "[ 3.78913043e-01 -9.93804348e+02  1.73913043e-01  2.39130435e-01\n",
      "  1.95652174e-01  2.39130435e-01  1.52173913e-01]\n",
      "[ 9.28571429e-01 -2.49885714e+03  0.00000000e+00  1.42857143e-01\n",
      "  5.71428571e-01  2.85714286e-01  0.00000000e+00]\n",
      "[ 7.385808e+03 -6.000000e-01  0.000000e+00  8.000000e-01  2.000000e-01\n",
      "  0.000000e+00  0.000000e+00]\n",
      "[2.60928899e+02 8.85245902e-01 1.94379391e-01 2.90398126e-01\n",
      " 2.31850117e-01 1.24121780e-01 1.52224824e-01]\n",
      "[ 0.000e+00 -5.368e+03  0.000e+00  0.000e+00  0.000e+00  1.000e+00\n",
      "  0.000e+00]\n"
     ]
    }
   ],
   "source": [
    "# 예측\n",
    "predictions = kmModel.transform(transformedTest)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = kmModel.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
