{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9978cbe6",
   "metadata": {},
   "source": [
    "# 3. 구조적 스트리밍\n",
    "---\n",
    "\n",
    "구조적 스트리밍은 스트림 처리용 고수준 API이다. 구조적 스트리밍을 사용하면 구조적 API로 개발된 배치 모드의 연산을 스트리밍 방식으로 실행할 수 있고 지연 시간을 줄이고 증분 처리할 수 있다. \n",
    "\n",
    "구조적 스트리밍의 간단한 예제를 봐보자.\n",
    "데이터 셋은 특정 날짜, 시간 정보 존재하는 소매 데이터(data/retail-data/by-day)를 사용한다.\n",
    "여러 프로세스에서 데이터가 **꾸준히 생성되는** 상황을 상상하라. 생성된 데이터가 구조적 스트리밍 잡이 읽을 수 있는 저장소로 전송되고 있다고 가정하자.\n",
    "\n",
    "먼저 정적 데이터 셋의 데이터를 분석해 dataframe을 생성하자. 그리고 정적 데이터셋의 스키마도 함께 생성하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4b0343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'UnitPrice', 'CustomerID', 'Country']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SparkSession for section3\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "staticDataFrame = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"..//data/retail-data/by-day/*.csv\")\n",
    "\n",
    "print(staticDataFrame.columns)\n",
    "\n",
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6805da71",
   "metadata": {},
   "source": [
    "지금은 시계열 데이터를 다루기 때문에 데이터를 그룹화하고 집계해야하므로 CustomerID가 대량으로 구매하는 영업 시간을 살펴볼 것이다.\n",
    "\n",
    "window 함수는 집계 시에 시계열 컬럼을 기준으로 각 날짜에 대한 전체 데이터를 가지는 윈도우를 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f24e27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------+\n",
      "|CustomerId|              window|  sum(total_cost)|\n",
      "+----------+--------------------+-----------------+\n",
      "|   16057.0|{2011-12-05 09:00...|            -37.6|\n",
      "|   14126.0|{2011-11-29 09:00...|643.6300000000001|\n",
      "|   13500.0|{2011-11-16 09:00...|497.9700000000001|\n",
      "|   17160.0|{2011-11-08 09:00...|516.8499999999999|\n",
      "|   15608.0|{2011-11-11 09:00...|            122.4|\n",
      "+----------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "\n",
    "staticDataFrame\\\n",
    "  .selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"(UnitPrice * Quantity) as total_cost\",\n",
    "    \"InvoiceDate\")\\\n",
    "  .groupBy(\n",
    "    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "  .sum(\"total_cost\")\\\n",
    "  .show(5)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337a6297",
   "metadata": {},
   "source": [
    "이제 스트리밍 코드를 보자. 코드는 거의 바뀌지 않지만 **readStream** 메서드를 사용하는게 큰 특징이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eed130cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDataFrame = spark.readStream\\\n",
    "    .schema(staticSchema)\\\n",
    "    .option(\"maxFilesPerTrigger\", 1)\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(\"/data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c501ad",
   "metadata": {},
   "source": [
    "정적 데이터셋 처리와 동일한 로직을 적용하고 스트리밍 액션을 호출하자.\n",
    "스트리밍 액션은 데이터를 어딘가에 채워넣어야 하므로 count 메서드와 같은 정적 액션과는 조금 다르다. 이번에 사용할 스트리밍 액션은 **트리거**가 실행된 다음, 데이터를 갱신하게 될 **인메모리 테이블**에 데이터를 저장한다. \n",
    "\n",
    "* 트리거\n",
    "\n",
    "    테이블에 INSERT 나 UPDATE 또는 DELETE 작업이 발생되면 자동으로 실행되는 코드를 말하는 것 같다. (인터넷 검색 참고)\n",
    "    \n",
    "* 인메모리 테이블\n",
    "\n",
    "    테이블의 데이터를 메모리로 올려, 모든 데이터 조작을 메모리상에서만 하도록 하는 방식을 얘기함. 디스크를 거치지 않기 때문에 매우 빠른 속도로 진행되지만, 모든 데이터가 메모리상에 있어야 하기 때문에 메모리 사용률이 증가한다. ([출처](https://mozi.tistory.com/566#:~:text=%EC%9D%B8%EB%A9%94%EB%AA%A8%EB%A6%AC%20%ED%85%8C%EC%9D%B4%EB%B8%94,%EB%A9%94%EB%AA%A8%EB%A6%AC%20%EC%82%AC%EC%9A%A9%EB%A5%A0%EC%9D%B4%20%EC%A6%9D%EA%B0%80%ED%95%A9%EB%8B%88%EB%8B%A4.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd3fc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fb12df26cd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 지연 연산 코드\n",
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    "  .selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"(UnitPrice * Quantity) as total_cost\",\n",
    "    \"InvoiceDate\")\\\n",
    "  .groupBy(\n",
    "    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "  .sum(\"total_cost\")\n",
    "\n",
    "# 데이터 플로 실행\n",
    "purchaseByCustomerPerHour.writeStream\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"customer_purchases\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3519283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------------+\n",
      "|CustomerId|window|sum(total_cost)|\n",
      "+----------+------+---------------+\n",
      "+----------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "  SELECT *\n",
    "  FROM \n",
    "실제 운영 환경에선 위 방식들처럼 쓰지 않지만, 예제를 통해 구조적 스트리밍의 강력함을 보여준다.\n",
    "  ORDER BY `sum(total_cost)` DESC\n",
    "  \"\"\")\\\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05f82fb",
   "metadata": {},
   "source": [
    "음.. 뭔가 갱신되는 걸 계속 보여줄 것 같았는데.. 아무것도 뜨질 않는다..;  일단 이런게 있다고 생각하고 넘어가도록 하자.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
